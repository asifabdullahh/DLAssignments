{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXgJ6uT1NydQ"
   },
   "source": [
    "Assignment: Flowers Recognition <br>\n",
    "Dataset Description:<br>\n",
    "\n",
    "This dataset contains 4242 images of flowers.<br>\n",
    "The data collection is based on the data flicr, google images, yandex images.<br>\n",
    "You can use this datastet to recognize plants from the photo.<br>\n",
    "\n",
    "Attribute Information:<br>\n",
    "The pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.<br>\n",
    "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. <br>\n",
    "<b>Also explore how to resize images in tensorflow and then resize all the images to a same size. </b> <br>\n",
    "This is a Multiclass Classification Problem.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7vy-ktuOKJH"
   },
   "source": [
    "WORKFLOW : <br>\n",
    "Load Data <br>\n",
    "Split into 60 and 40 ratio.<br>\n",
    "Encode labels.<br>\n",
    "Create Model<br>\n",
    "Compilation Step (Note : Its a Multiclass Classification problem , select loss , metrics according to it)<br>\n",
    "Train the Model.<br>\n",
    "If the model gets overfit tune your model by changing the units , No. of layers , epochs , add dropout layer or add Regularizer according to the need .<br>\n",
    "Prediction should be > 85%<br>\n",
    "Evaluation Step<br>\n",
    "Prediction<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri3Bg5qfPRic"
   },
   "source": [
    "Data : <br>\n",
    "https://drive.google.com/file/d/1-OX6wn5gA-bJpjPNfSyaYQLz-A-AB_uj/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hTtg3WuGTA1o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "daisy_path = \"flowerdata/flowers/flowers/daisy/\"\n",
    "dandelion_path = \"flowerdata/flowers/flowers/dandelion/\"\n",
    "rose_path = \"flowerdata/flowers/flowers/rose/\"\n",
    "sunflower_path = \"flowerdata/flowers/flowers/sunflower/\"\n",
    "tulip_path = \"flowerdata/flowers/flowers/tulip/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "img_data = []\n",
    "labels = []\n",
    "\n",
    "size = 128,128\n",
    "def iter_images(images,directory,size,label):\n",
    "    try:\n",
    "        for i in range(len(images)):\n",
    "            img = cv2.imread(directory + images[i])\n",
    "            img = cv2.resize(img,size)\n",
    "            img_data.append(img)\n",
    "            labels.append(label)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "iter_images(listdir(daisy_path),daisy_path,size,0)\n",
    "iter_images(listdir(dandelion_path),dandelion_path,size,1)\n",
    "iter_images(listdir(rose_path),rose_path,size,2)\n",
    "iter_images(listdir(sunflower_path),sunflower_path,size,3)\n",
    "iter_images(listdir(tulip_path),tulip_path,size,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4323, 4323)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_data),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.asarray(img_data)\n",
    "\n",
    "#div by 255\n",
    "data = data / 255.0\n",
    "\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4323, 128, 128, 3), (4323,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.30, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "24/24 [==============================] - 225s 9s/step - loss: 1.5409 - accuracy: 0.2677 - val_loss: 1.2893 - val_accuracy: 0.4402\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - 171s 7s/step - loss: 1.1913 - accuracy: 0.4918 - val_loss: 1.1520 - val_accuracy: 0.4958\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - 196s 8s/step - loss: 1.0423 - accuracy: 0.5487 - val_loss: 1.0960 - val_accuracy: 0.5559\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - 179s 7s/step - loss: 0.9945 - accuracy: 0.6174 - val_loss: 1.0934 - val_accuracy: 0.5212\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - 174s 7s/step - loss: 0.9032 - accuracy: 0.6583 - val_loss: 1.0705 - val_accuracy: 0.5682\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 161s 7s/step - loss: 0.8650 - accuracy: 0.6640 - val_loss: 1.0340 - val_accuracy: 0.5675\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - 146s 6s/step - loss: 0.7762 - accuracy: 0.7144 - val_loss: 1.0464 - val_accuracy: 0.5729\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 150s 6s/step - loss: 0.7118 - accuracy: 0.7323 - val_loss: 1.0265 - val_accuracy: 0.5821\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 156s 7s/step - loss: 0.6242 - accuracy: 0.7879 - val_loss: 1.0357 - val_accuracy: 0.5929\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 162s 7s/step - loss: 0.5286 - accuracy: 0.8226 - val_loss: 1.0392 - val_accuracy: 0.5837\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - 188s 8s/step - loss: 0.3769 - accuracy: 0.8800 - val_loss: 1.0985 - val_accuracy: 0.5883\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 144s 6s/step - loss: 0.3072 - accuracy: 0.9049 - val_loss: 1.2829 - val_accuracy: 0.5759\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 122s 5s/step - loss: 0.2444 - accuracy: 0.9374 - val_loss: 1.2488 - val_accuracy: 0.5960\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 163s 7s/step - loss: 0.1966 - accuracy: 0.9479 - val_loss: 1.2796 - val_accuracy: 0.5790\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 166s 7s/step - loss: 0.1640 - accuracy: 0.9653 - val_loss: 1.4010 - val_accuracy: 0.5806\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 172s 7s/step - loss: 0.1480 - accuracy: 0.9655 - val_loss: 1.4629 - val_accuracy: 0.5898\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 184s 8s/step - loss: 0.1019 - accuracy: 0.9787 - val_loss: 1.5196 - val_accuracy: 0.5837\n",
      "Epoch 19/30\n",
      "24/24 [==============================] - 170s 7s/step - loss: 0.0994 - accuracy: 0.9815 - val_loss: 1.5575 - val_accuracy: 0.5860\n",
      "Epoch 20/30\n",
      "24/24 [==============================] - 171s 7s/step - loss: 0.0656 - accuracy: 0.9905 - val_loss: 1.6198 - val_accuracy: 0.5906\n",
      "Epoch 21/30\n",
      "24/24 [==============================] - 170s 7s/step - loss: 0.0549 - accuracy: 0.9926 - val_loss: 1.7472 - val_accuracy: 0.5790\n",
      "Epoch 22/30\n",
      "24/24 [==============================] - 185s 8s/step - loss: 0.0481 - accuracy: 0.9931 - val_loss: 1.7553 - val_accuracy: 0.5798\n",
      "Epoch 23/30\n",
      "24/24 [==============================] - 181s 8s/step - loss: 0.0443 - accuracy: 0.9945 - val_loss: 1.8333 - val_accuracy: 0.5829\n",
      "Epoch 24/30\n",
      "24/24 [==============================] - 183s 8s/step - loss: 0.0304 - accuracy: 0.9958 - val_loss: 1.9667 - val_accuracy: 0.5682\n",
      "Epoch 25/30\n",
      "24/24 [==============================] - 185s 8s/step - loss: 0.0289 - accuracy: 0.9951 - val_loss: 2.0126 - val_accuracy: 0.5752\n",
      "Epoch 26/30\n",
      "24/24 [==============================] - 172s 7s/step - loss: 0.0275 - accuracy: 0.9947 - val_loss: 1.9536 - val_accuracy: 0.5813\n",
      "Epoch 27/30\n",
      "24/24 [==============================] - 164s 7s/step - loss: 0.0226 - accuracy: 0.9972 - val_loss: 2.0102 - val_accuracy: 0.5891\n",
      "Epoch 28/30\n",
      "24/24 [==============================] - 162s 7s/step - loss: 0.0180 - accuracy: 0.9975 - val_loss: 2.1175 - val_accuracy: 0.5744\n",
      "Epoch 29/30\n",
      "24/24 [==============================] - 172s 7s/step - loss: 0.0205 - accuracy: 0.9978 - val_loss: 2.3135 - val_accuracy: 0.5574\n",
      "Epoch 30/30\n",
      "24/24 [==============================] - 154s 6s/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 2.1560 - val_accuracy: 0.5929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2557b150a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (2,2),input_shape=(128, 128, 3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (2,2),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asif mughal\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ==> 3\n",
      "3 ==> 4\n",
      "1 ==> 1\n",
      "4 ==> 2\n",
      "3 ==> 3\n",
      "2 ==> 2\n",
      "2 ==> 2\n",
      "0 ==> 0\n",
      "1 ==> 1\n",
      "1 ==> 1\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes(x_test[:10])\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    print(pred[i],'==>',y_test[i])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Flowers Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
